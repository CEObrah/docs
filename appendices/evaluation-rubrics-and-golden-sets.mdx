---
title: "Appendix C: Evaluation Rubrics & Golden Sets"
description: "Our methodology for creating evaluation rubrics and golden sets."
---

Our evaluation process is built on a foundation of high-quality, human-curated data. This appendix provides a brief overview of our methodology for creating evaluation rubrics and golden sets.

## Evaluation Rubrics

For complex, subjective tasks, we use evaluation rubrics to measure the quality of our agents' outputs. These rubrics are created by a team of in-house experts, and they are designed to be as objective and comprehensive as possible.

Each rubric includes a set of criteria, as well as a scoring guide that explains how to apply the criteria to a given output.

## Golden Sets

For tasks where there is a single "right" answer, we use "golden sets" to measure the accuracy of our agents. These golden sets are created by a team of in-house experts, and they are designed to be as comprehensive as possible.

Each golden set includes a set of inputs, as well as the corresponding correct outputs.

## Sampling Method

We use a variety of sampling methods to ensure that our evaluation data is representative of the real-world tasks that our customers are performing on our platform. These methods include:

-   **Random Sampling:** We randomly sample from our customers' data to create a representative dataset.
-   **Stratified Sampling:** We use stratified sampling to ensure that our dataset includes a representative sample of all the different types of tasks that our customers are performing.
-   **Adversarial Sampling:** We use adversarial sampling to create a dataset that is specifically designed to test the limits of our platform.